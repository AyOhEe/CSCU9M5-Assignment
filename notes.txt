This .txt file contains the rough contents for each section, before I made the poster.
I prefer writing my thoughts in text, but writing them in a file lets me keep this in version control.

Section 1:
3423815

Section 2:
[TODO progressively outline]
Test/train/validate: 65/17.5/17.5
This was done to ensure sufficient test and validation data at the end of training, as
the dataset is rather small at only 517 entries.

Performed histogram analysis and removed "rain" variable.
Configured orange workflow to train and test the three selected model types on the train 
and test datasets, and to display results on both the test and validation datasets.

Experimented with variations of hyperparameters for all three chosen model types,
eventually settling on [TODO model] with [TODO hyperparameters]

Section 3:
- X: Discrete Feature - Used
- Y: Discrete Feature - Used
- Month: Categorical Feature - Used
- Day: Categorical Feature - Unused
- FFMC: Continuous Feature - Used
- DMC: Continuous Feature - Used
- DC: Continuous Feature - Used
- ISI: Continuous Feature - Used
- Temp: Continuous Feature - Used
- RH: Continuous Feature - Used
- Wind: Continuous Feature - Used
- Rain: Continuous Feature - Unused (due to histogram)
- Area: Categorical Target - Required.
TODO [explain consequence]
 - Something about assuming that the main correlation to be found in position will be identifying
   particularly well-burning "hot spots" for wildfires, and as such one-hot encoding will make more
   sense than continuous as the correlation is assumed to not be linear w.r.t. X or Y, but a
   combination involving both coordinates.
 - That isn't really a consequence. Think of something else?


Section 4:
The "rain" variable has a large majority value at 0, comprising almost all datapoints.
As such, this variable was not considered.

Section 5:
Model types used: Neural Network, Random forest, Support Vector Machine
All models were evaluated with 5-fold cross validation

Neural network hyperparameter experiments:
- CA = 0.733 @ [100,] with 500 iterations
- CA = 0.786 @ [100,100] with 500 iterations
- CA = 0.816 @ [100,100] with 1000 iterations
- CA = 0.843 @ [200, 100] with 1000 iterations
- CA = 0.807 @ [200, 100, 50] with 1000 iterations
- CA = 0.804 @ [200, 100, 50] with 2000 iterations
- CA = 0.816 @ [200, 100] with 2000 iterations
- CA = 0.763 @ [200, 200] with 1000 iterations
- CA = 0.780 @ [300, 100] with 1000 iterations
- CA = 0.777 @ [300, 100] with 2000 iterations
- CA = 0.772 @ [300, 200] with 3000 iterations
- CA = 0.825 @ [200, 100] with 1500 iterations

Best performing parameters: [200, 100] with 1000 iterations, for CA = 0.843


Random forest hyperparameter experiments:
- CA = 0.831 @ n=10, threshold=5
- CA = 0.846 @ n=20, threshold=5
- CA = 0.843 @ n=30, threshold=5
- CA = 0.858 @ n=25, threshold=5
- CA = 0.813 @ n=5, threshold=5
- CA = 0.834 @ n=25, threshold=8
- CA = 0.843 @ n=25, threshold=3

Best performing parameters: n=26, threshold=5, for CA = 0.858


SVM hyperparameter experiments:
- CA = 0.754 @ C=1.00, e=0.1, iterations=100
- CA = 0.751 @ C=1.00, e=0.1, iterations=200
- CA = 0.774 @ C=2.00, e=0.1, iterations=200
- CA = 0.766 @ C=2.00, e=0.1, iterations=100
- CA = 0.766 @ C=4.00, e=0.1, iterations=200
-----Switched to RBF-----
- CA = 0.804 @ C=2.00, e=0.1, iterations=200, g=auto, c=1.0, d=4
- CA = 0.804 @ C=2.00, e=0.1, iterations=300, g=auto, c=1.0, d=4
- CA = 0.813 @ C=2.00, e=0.1, iterations=300, g=auto, c=1.0, d=5
- CA = 0.783 @ C=2.00, e=0.1, iterations=300, g=auto, c=1.5, d=5
- CA = 0.769 @ C=2.00, e=0.1, iterations=300, g=auto, c=2.0, d=5

Best performing parameters: C=2.00, e=0.1, iterations=300, g=auto, c=1.0, d=5, for CA = 0.813


The best model seems to be Random Forest, with a CA of 0.858 with n=25, threshold=5


Section 6:
Final model chosen: [TODO fill]
Justification: [TODO fill]
Confusion matrix on validation dataset: [TODO fill]
        Predicted
        F          T
A
c  F
t
u
a  T
l

Comment: [TODO write]


Section 7:
[TODO insight]
[TODO model features?]
[TODO bias?]
[TODO useful?]

Section 8:
[TODO convert to IEEE] Canvas materials by Sandy Brownlee